{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d02ec655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import os\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af4f7e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /\n"
     ]
    }
   ],
   "source": [
    "# current working directory\n",
    "cwd = os.getcwd()\n",
    "#set wroking direvtly to one level up\n",
    "os.chdir(os.path.dirname(cwd))\n",
    "# rpint new working directory\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e66e52",
   "metadata": {},
   "source": [
    "## Please update the URL swith the lastest ons change file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c874f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Extracted database date: 2023-12\n",
      "Extracting ChangeHistory.csv to gsscoder_python/lookups/\n",
      "Extracting Changes.csv to gsscoder_python/lookups/\n",
      "✅ Both files downloaded and extracted to gsscoder_python/lookups\n"
     ]
    }
   ],
   "source": [
    "# === Step 1: Define download URL and output path ===\n",
    "url = \"https://www.arcgis.com/sharing/rest/content/items/3acc892515aa49a8885c2deb734ebd3d/data\"\n",
    "output_dir = os.path.join(\"gsscoder_python\", \"lookups\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Step 2: Download the ZIP file ===\n",
    "response = requests.get(url)\n",
    "if not response.ok:\n",
    "    raise Exception(f\"Download failed with status code {response.status_code}\")\n",
    "\n",
    "# === Step 3: Extract only the relevant CSV files ===\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "    # Identify any CSVs and try to extract the database name\n",
    "    extracted_files = []\n",
    "    zip_file_names = z.namelist()\n",
    "\n",
    "    # Try to identify the name of the database file for date extraction\n",
    "    db_file_name = next((f for f in zip_file_names if \"Code_History_Database\" in f), None)\n",
    "    if db_file_name:\n",
    "        # Try to extract the month and year from the bracketed part\n",
    "        match = re.search(r\"\\(([^)]+)\\)\", db_file_name)\n",
    "        if match:\n",
    "            raw_date_str = match.group(1).strip().replace(\"_\", \" \")  # Fix underscores like \"December_2023\" -> \"December 2023\"\n",
    "            try:\n",
    "                parsed_date = datetime.strptime(raw_date_str, \"%B %Y\")\n",
    "                database_date = parsed_date.strftime(\"%Y-%m\")\n",
    "                print(f\"📅 Extracted database date: {database_date}\")\n",
    "            except ValueError:\n",
    "                print(\"⚠️ Could not parse date string:\", raw_date_str)\n",
    "                database_date = None\n",
    "        else:\n",
    "            print(\"⚠️ No date in brackets found in filename.\")\n",
    "            database_date = None\n",
    "    else:\n",
    "        print(\"⚠️ Could not find a 'Code_History_Database' file in ZIP.\")\n",
    "        database_date = None\n",
    "\n",
    "    # Extract Changes.csv and ChangeHistory.csv\n",
    "    extracted_files = [f for f in zip_file_names if f.endswith(\"Changes.csv\") or f.endswith(\"ChangeHistory.csv\")]\n",
    "    if not extracted_files:\n",
    "        raise Exception(\"Neither Changes.csv nor ChangeHistory.csv was found in the ZIP file.\")\n",
    "\n",
    "    for file_name in extracted_files:\n",
    "        print(f\"Extracting {file_name} to {output_dir}/\")\n",
    "        with z.open(file_name) as source_file:\n",
    "            destination_path = os.path.join(output_dir, os.path.basename(file_name))\n",
    "            with open(destination_path, 'wb') as target_file:\n",
    "                target_file.write(source_file.read())\n",
    "\n",
    "print(\"✅ Both files downloaded and extracted to gsscoder_python/lookups\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de91c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-12'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff53ce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 2: Read input files ===\n",
    "\n",
    "changes_path = \"gsscoder_python/lookups/Changes.csv\"\n",
    "history_path = \"gsscoder_python/lookups/ChangeHistory.csv\"\n",
    "\n",
    "code_changes = pd.read_csv(changes_path, dtype=str)\n",
    "change_history = pd.read_csv(history_path, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8d9301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 2: Prompt for and validate Code History Database Date ===\n",
    "\n",
    "database_date_input = database_date\n",
    "\n",
    "# Ensure date input is in correct format\n",
    "while not re.match(r\"^\\d{4}-\\d{2}$\", database_date_input):\n",
    "    database_date_input = input(\"Unexpected format for Code History Database Date entered. Please try again with format yyyy-mm: \")\n",
    "\n",
    "# Assume end of the month for date\n",
    "database_date = pd.to_datetime(f\"{database_date_input}-31\")\n",
    "\n",
    "# Check for implausible dates\n",
    "if database_date > pd.Timestamp.today():\n",
    "    warnings.warn(\"The Code History Database Date given is in the future. This is unlikely to be correct, please double check.\")\n",
    "\n",
    "if database_date < pd.Timestamp(\"2009-01-01\"):\n",
    "    sys.exit(\"The Code History Database Date cannot be any earlier than 2009-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1218ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 3: LAD filter ===\n",
    "\n",
    "geogs_of_interest = [\"E06\", \"E07\", \"E08\", \"E09\", \"W06\"]\n",
    "\n",
    "# Keep rows where ENTITYCD and GEOGCD_P are both LAD-level\n",
    "lad_code_changes = code_changes[code_changes[\"ENTITYCD\"].isin(geogs_of_interest)].copy()\n",
    "lad_code_changes[\"old_entity\"] = lad_code_changes[\"GEOGCD_P\"].str[:3]\n",
    "lad_code_changes = lad_code_changes[lad_code_changes[\"old_entity\"].isin(geogs_of_interest)]\n",
    "\n",
    "# Drop Welsh name columns\n",
    "lad_code_changes.drop(columns=[\"GEOGNMW\", \"GEOGNMW_P\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de118f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 4: Detect Merges and Splits ===\n",
    "\n",
    "# Merges: Multiple rows with same GEOGCD (new code)\n",
    "merges = lad_code_changes[\"GEOGCD\"].value_counts()\n",
    "merges = merges[merges > 1].index.tolist()\n",
    "\n",
    "# Splits: Multiple rows with same GEOGCD_P (old code)\n",
    "splits = lad_code_changes[\"GEOGCD_P\"].value_counts()\n",
    "splits = splits[splits > 1].index.tolist()\n",
    "\n",
    "lad_code_changes[\"split\"] = lad_code_changes[\"GEOGCD_P\"].isin(splits)\n",
    "lad_code_changes[\"merge\"] = lad_code_changes[\"GEOGCD\"].isin(merges)\n",
    "\n",
    "# Parse operational date\n",
    "lad_code_changes[\"OPER_DATE\"] = pd.to_datetime(lad_code_changes[\"OPER_DATE\"], format=\"%d/%m/%Y\")\n",
    "\n",
    "# Rename and select relevant columns\n",
    "lad_code_changes = lad_code_changes.rename(columns={\n",
    "    \"GEOGCD\": \"changed_to_code\",\n",
    "    \"GEOGNM\": \"changed_to_name\",\n",
    "    \"GEOGCD_P\": \"changed_from_code\",\n",
    "    \"GEOGNM_P\": \"changed_from_name\",\n",
    "    \"SI_TITLE\": \"desc\",\n",
    "    \"ENTITYCD\": \"entity_type\",\n",
    "    \"OPER_DATE\": \"date\",\n",
    "    \"YEAR\": \"year\"\n",
    "})[[\n",
    "    \"changed_to_code\", \"changed_to_name\", \"changed_from_code\", \"changed_from_name\",\n",
    "    \"desc\", \"entity_type\", \"date\", \"year\", \"split\", \"merge\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b5261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 5: Handle Minor Boundary Changes ===\n",
    "\n",
    "minor_changes = {\n",
    "    (\"E08000020\", \"E06000057\"),  # Gateshead → Northumberland\n",
    "    (\"E07000097\", \"E07000243\"),  # East Hertfordshire → Stevenage\n",
    "    (\"W06000007\", \"W06000024\"),  # Powys → Merthyr Tydfil\n",
    "}\n",
    "minor_descs = [\n",
    "    \"The Gateshead and Northumberland (Boundary Change) Order 2013\",\n",
    "    \"The East Hertfordshire and Stevenage (Boundary Change) Order 2013\",\n",
    "    \"The Merthyr Tydfil and Powys (Areas) Order 2009\"\n",
    "]\n",
    "\n",
    "# Drop specific boundary transfer rows\n",
    "lad_code_changes = lad_code_changes[\n",
    "    ~lad_code_changes[[\"changed_from_code\", \"changed_to_code\"]].apply(tuple, axis=1).isin(minor_changes)\n",
    "]\n",
    "\n",
    "# Set split and merge = False for rows with matching descriptions\n",
    "lad_code_changes.loc[lad_code_changes[\"desc\"].isin(minor_descs), [\"split\", \"merge\"]] = False\n",
    "\n",
    "# Warn if new splits exist that aren't in minor list\n",
    "new_splits = lad_code_changes[lad_code_changes[\"split\"] == True]\n",
    "if not new_splits.empty:\n",
    "    print(new_splits)\n",
    "    print(\"\\nWARNING: New splits found that may require manual review.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d07860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 6: Prepare all LAD code history table ===\n",
    "\n",
    "change_history = change_history[change_history[\"ENTITYCD\"].isin(geogs_of_interest)].copy()\n",
    "\n",
    "change_history[\"start_date\"] = pd.to_datetime(change_history[\"OPER_DATE\"], format=\"%d/%m/%Y\", errors=\"coerce\")\n",
    "change_history[\"end_date\"] = pd.to_datetime(change_history[\"TERM_DATE\"], format=\"%d/%m/%Y\", errors=\"coerce\")\n",
    "\n",
    "all_lad_codes_dates = change_history.rename(columns={\n",
    "    \"GEOGCD\": \"gss_code\",\n",
    "    \"GEOGNM\": \"gss_name\",\n",
    "    \"SI_TITLE\": \"desc\",\n",
    "    \"ENTITYCD\": \"entity_type\",\n",
    "    \"STATUS\": \"status\"\n",
    "})[[\n",
    "    \"gss_code\", \"gss_name\", \"desc\", \"entity_type\", \"start_date\", \"end_date\", \"status\"\n",
    "]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d4797b",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'gsscoder_python'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# === Step 7: Save Output Files ===\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgsscoder_python/lookups\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m lad_code_changes\u001b[38;5;241m.\u001b[39mto_pickle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgsscoder_python/lookups/lad_code_changes.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Py equivalent of RDS\u001b[39;00m\n\u001b[1;32m      5\u001b[0m all_lad_codes_dates\u001b[38;5;241m.\u001b[39mto_pickle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgsscoder_python/lookups/all_lad_codes_dates.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'gsscoder_python'"
     ]
    }
   ],
   "source": [
    "# === Step 7: Save Output Files ===\n",
    "\n",
    "os.makedirs(\"gsscoder_python/lookups\", exist_ok=True)\n",
    "lad_code_changes.to_pickle(\"gsscoder_python/lookups/lad_code_changes.pickle\")  # Py equivalent of RDS\n",
    "all_lad_codes_dates.to_pickle(\"gsscoder_python/lookups/all_lad_codes_dates.pickle\")\n",
    "with open(\"gsscoder_python/lookups/database_date.txt\", \"w\") as f:\n",
    "    f.write(str(database_date))\n",
    "\n",
    "print(\"✅ Lookup tables saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
